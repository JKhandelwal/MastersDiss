\section{Introduction}
Using geopolitical news to predict stock prices would be of great interest to investors. If, by mining geopolitical news predictions are possible and accurate with regards to the direction the stock market will go, it would mean investors will be able to not only not \textit{lose} money, e.g. by selling in a bear market before the price crashes, but in certain circumstances, be able to make money, e.g. buying in a bull market before the price increases.

This dissertation was focused around using the Global Database of Events, Language, and Tone (GDELT) to build topic models which could be used to predict the changes in the stock market. The topic models and the GDELT data used was related to geopolitical information. The aim was to use GDELT data to build a topic model which could be used to filter news information for specifically geopolitical news. Then the geopolitical measure of the conflict cooperation Goldstein Scale for geopolitical events provided by GDELT would be used to try and predict information on the stock market.

The methodology behind building a topic model took news headlines from the GDELT data over a period of time, and then performed cluster analysis/topic curation on the headlines themselves, with the aim being to tease out the underlying topics, which would ideally be specifically focused on the geopolitical topics. The next step was to use the curated topic model to filter other news to end up with headlines specifically related to that geopolitical topic. The other news would most likely be new daily news published daily. 

The initial approach to building the topic models was to use existing topic modelling algorithms. The first topic model used was the Latent Dirichlet Analysis on data from the GDELT Events Table, filtered to data which which was specific to events concerning the USA or China. The aim for this data would be to sort all of the words present into K topics. USA and China specific data was chosen for two reasons. Firstly it was to narrow down the dataset, as GDELT is a very large resource, and it would be more feasible initially to run on a smaller dataset. The second reason for this is using geopolitical news would have to be targeted considerably to certain events in specific countries and particular markets. Over the time period from March to April, there were significant events which took place specifically between those two countries, and it was thought this would provide the best opportunity to find signal within the data. 

For a topic model to filter other news, there would be a binary or qualitative measure produced when a specific item of news is tested in topic model. If binary, this would either say the news item is part of the topic or is not. A qualitative measure, referred to as the distance, would provide a numerical value which would represent how `far` away a news item is from the topic in the topic model, with smaller values suggesting the news item is related to the topic in the topic model, and larger values suggesting the news item is less related to the topic in the topic model. The LDA model did not provide a binary metric, and was not able to provide a qualitative measure for news which included words not already present in the topic model. Thus, a different approach was used to represent topics, which would be able to provide qualitative measures.  

The K Means clustering algorithm was used alongside the Term Frequency Inverse Document Frequency (TF-IDF) to build clusters from the words in the headlines. This approach used TF-IDF to represent the words numerically, such that the K Means algorithm could be used. This would mean that the clusters would be built based on words and phrases which occur frequently. It was hoped those clusters would represent the words and the phrases which are relevant to geopolitical information, and thus it could be used. 

This approach however also did not work as intended. The TF-IDF vectorisation and numerical transformation meant that news unrelated to the topic models often ended up with the same qualitative distance from the cluster centres as news which was related to the underlying topics. This meant that the K Means model could not be used to filter news for relevant topics. 

The initial aim was to use a topic modelling algorithm trained on USA/China to filter all of the news provided by GDELT to news which was just relevant to the main topics present in the USA/China data. However, since neither of the topic modelling approaches tried were able to filter the information correctly, the filtered dataset used by the stock prediction models was the same as the dataset used for the topic modelling algorithms, which was a dataset filtered in GDELT itself to only be about news concerning the USA or China. 

The models for stock prediction modelled the Dow Jones Industrial Average. This is a market index which represent the averaged performance of 30 large USA companies. This was chosen again because geopolitical risk prediction would be done on specific markets, and it was thought that any events concerning the USA and China would be reflected into the Dow Jones. When making predictive models, stock shifts, meaning here a binary increase or decrease of the index on a daily basis, was used for predictions as opposed to predicting exact stock index value. It would also be assumed that the influence on the stock prices and thus the Dow Jones from a geopolitical event was considered to potentially extend over several days, and thus any models for prediction would have to take this into consideration.

GDELT provides two qualitative values in the data, the Goldstein Scale and the Average Tone. Both of these describe the news articles and the events that they are describing and are explained more fully in chapter \ref{gdelt}. These two were the main features used in modelling the Dow Jones Average, alongside using the previous day's . Many different classification models were tried, including Random Forests Classifiers, Support Vector Machines (SVMs), Logistic Regression, and a more Bayesian approach in Naive Bayes. Furthermore a simple Multi Layer Perceptron Neural Network was also tried. Several different methods of introducing lag into the Goldstein Scale and Average Tone were also tried, these are explained further in chapter \ref{experiments}, and used alongside the models listed. 

Since the prediction models were predicting a binary change in the stock price index, the best model was picked on the basis of the model with highest accuracy, which meant the model which predicted whether the price went up or down correctly for the highest number of days over the period of time the models were trained on. This was a Random Forests Classifier using exponential weightings. This model was further tested over a longer time interval by predicting the stock index shift in the interval of May and June 2020. To gauge whether the Average Tone and Goldstein Score values were useful in predicting the shift, a reference Random Forests model was trained which just used the previous day's change as a predictor. This reference model was also tested over the May-June interval, and it was found that the `best` Random Forests model accrued a higher accuracy than the reference model over the prediction interval.

There remains significant further modelling possible to explore both GDELT and topic modelling further, both in terms of the scale of the data being predicted, and different markets and modelling strategies. 

This report will cover relevant background literature in chapters \ref{prior}, \ref{gdelt}, and \ref{topic};  an overview of the experiments performed and results in chapters \ref{experiments} and \ref{results}, and a discussion of the results in chapter \ref{conclusions}.