\section{Introduction}
There are many geopolitical events occurring worldwide, which may have a knock on affect on stock markets. To be able to find and predict the impact such events would have on markets would be of great interest to investors. If, by mining the geopolitical news these events are reported in, mathematical predictions are possible and accurate with regards to the direction the stock market will go, it would mean investors will be able to not only not \textit{lose} money, e.g. by selling in a bear market before the price crashes, but in certain circumstances, be able to make money, e.g. buying in a bull market before the price increases.

This dissertation was focused around using the Global Database of Events, Language, and Tone (GDELT) to build topic models which could be used with models which predict the changes in the stock market. The topic models and the GDELT data used was related to geopolitical information. The aim was to use GDELT data to build a topic model which could be used to filter news information for specifically geopolitical news. Then the geopolitical measure of the conflict cooperation Goldstein Scale for geopolitical events provided by GDELT would be used to try and predict information on the stock market.

The methodology of curating topic models from geopolitical news involved taking news headlines from GDELT data over a defined period of time. Then the underlying topics present in the news were teased out by grouping the words in the headlines together using different techniques. The aim for the underlying topics which emerge from word groupings was that they would be focused on and related to geopolitical topics. The next step was to use the curated topic model to filter other news to end up with headlines specifically related to that geopolitical topic. The other news would be `new` news which the model had not seen before, which would in the real world represent the daily news cycle.

The initial approach to building the topic models was to use existing topic modelling algorithms. The first topic model used was Latent Dirichlet Allocation. This was run on data from the GDELT events table, which was specific to events which concerned the USA or China. The aim for this data would be to sort all of the words present into a predefined number of topics, henceforth referred to as K, this is selected by the user before building a model. USA and China specific data was chosen for two reasons. Firstly it was to narrow down the dataset, as GDELT is a very large resource, and it would be more feasible initially to run on a smaller dataset. The second reason for this is using geopolitical news would have to be targeted considerably to certain events in specific countries and particular markets. Over the time period from March to April, there were significant events which took place specifically between those two countries, and it was thought this would provide the best opportunity to find signal within the data. 

For a topic model to filter other news, there would be a binary or quantitative measure produced when a specific item of news is tested in topic model. If binary, this would either say the news item is part of the topic or is not. A quantitative measure, referred to as the distance, would provide a numerical value which would represent how `far` away a news item is from the topic in the topic model, with smaller values suggesting the news item is related to the topic in the topic model, and larger values suggesting the news item is less related to the topic in the topic model. The LDA model did not provide a binary metric, and was not able to provide a quantitative measure for news which included words not already present in the topic model. Thus, a different approach was used to represent topics, which would be able to provide quantitative measures.  

The K-Means clustering algorithm was used alongside the Term Frequency Inverse Document Frequency (TF-IDF) to build clusters from the words in the headlines. This approach used TF-IDF to represent the words numerically, such that the K-Means algorithm could be used. This is explained more fully in chapter \ref{topic}. This would mean that the clusters would be built based on words and phrases which occur frequently and were important in the text. It was hoped those clusters would represent the words and the phrases which are relevant to geopolitical information. Whilst this approach was able to provide a quantitative distance, the actual distance values produced were unusable as they did not reliably filter topic relevant news from irrelevant news or noise. 

The initial aim was to use a topic modelling algorithm trained on USA/China to filter all of the news provided by GDELT to news which was just relevant to the main topics present in the USA/China data. Neither of the topic modelling approaches could filter `new` news headlines accurately. Thus the dataset used to build the stock prediction models was the same USA/China dataset used to build the topic models. 

The models for stock prediction modelled the Dow Jones Industrial Average. This is a market index which represents the averaged stock performance of 30 large USA companies on the NASDAQ and the New York Stock Exchange stock markets. This was chosen again because geopolitical risk prediction would be done on specific markets, and it was thought that any events concerning the USA and China would be reflected into the Dow Jones. When making predictive models, stock shifts, meaning here a binary increase or decrease of the index on a daily basis, was used for predictions as opposed to predicting exact stock index value. It would also be assumed that the influence on the stock prices and thus the Dow Jones from a geopolitical event was considered to potentially extend over several days, and thus any models for prediction would have to take this into consideration.

GDELT provides two quantitative values in the data, the Goldstein Scale and the Average Tone. Both of these describe the news articles and the events that they are describing and are explained more fully in chapter \ref{gdelt}. These two were the main features used in modelling the Dow Jones Average, alongside using the previous day's stock shift. Many different classification models were tried, including Random Forests Classifiers, Support Vector Machines (SVMs), Logistic Regression, and a more Bayesian approach in Naive Bayes. Furthermore a simple Multi Layer Perceptron Neural Network was also tried. Several different methods of introducing lag into the Goldstein Scale and Average Tone were also tried, these are explained further in chapter \ref{experiments}, and used alongside the models listed. 

Since the prediction models were predicting a binary change in the stock price index, the best model was picked on the basis of the model with highest accuracy, which meant the model which predicted whether the price went up or down correctly for the highest number of days over the period of time the models were trained on. This was a Random Forests Classifier using exponential lagging. This model was further tested over a longer time interval by predicting the stock index shift in the interval of May and June 2020. To gauge whether the Average Tone and Goldstein Score values were useful in predicting the shift, a reference Random Forests model was trained which just used the previous day's stock change as a predictor. This reference model was also tested over the May-June interval, and it was found that the `best` Random Forests model accrued a higher accuracy than the reference model over the prediction interval. Furthermore, a `real life` test was also performed for the best model and the reference model, where they were used as predictors in managing a simple portfolio, and it was found that if the model was used, the portfolio did grow in value over the May-June interval.  

There remains significant further modelling possible to explore both GDELT and topic modelling further, both in terms of the scale of the data being predicted, and different markets and modelling strategies. 

This report will cover relevant background literature in chapters \ref{prior}, \ref{gdelt}, and \ref{topic};  an overview of the experiments performed and results in chapters \ref{experiments} and \ref{results}, and a discussion of the results in chapter \ref{conclusions}.