\section{Discussion}
The broader purpose of topic modelling would be to build a topic model which would be able to categorise new and upcoming geopolitical risks, and thus be able to filter through only the relevant information. In the context of using GDELT, it would mean filtering out the headlines to only use the tone and Goldstein scale. This would mean that the topic model would not only have to be able to curate existing risks adequately, but also be able to catch new emerging risks too.

After performing the cluster analysis, and modelling, there were several things which became apparent. Firstly, the topic models are good at finding the main topics. Both the clusters and the LDA topic models were able to find what would be considered the `main` words in the topic, in that if a human were to look for the most important words, this is what they would be. Furthermore, if the Tf-IDF results were examined, the results were surprisingly insightful into the data. The results were mostly coherent and representative of what a human would retrospectively brainstorm for the time period of the training data.

However, one of the main concerns for both the LDA model and the clustering algorithm is the lack of separability. In both of the topic modelling attempts it appeared that there was only ever one topic being detected, in that there was no separation between the words such as . This could perhaps be a result of there not being at least two topics present in the data. However, comparing the single day data and the USA China data, neither set of word clouds strongly show a prevailing topic, though both represent the data reasonably. This in itself is extremely promising, as the corpus and documents themselves were composed of URLs which were brute force parsed, which in turn meant that there were a lot of `garbage` documents, the result of parsed URLs where the headline wasn't coherent or even not there.

The main concern with the topic models is the fact that existing topic models cannot be used effectively in terms of filtering new information, and thus cannot detect new topics. One of the biggest issues with LDA models is that the documents themselves have been generated from a specific set of topics, which means that if the word being filtered didn't exist in the corpus there is no probability/closeness metric that can generated for each topic (at least in the implementation that was used). This means that one of the only ways a topic model could be used for new data is if an entire dictionary was used in the corpus alongside the corpus. This in turn would create two other problems. Firstly, it would disrupt the meaning of the topics as there would be an excess of unrelated words to the documents. The second issue with this approach is more prohibitive. New words and topics appear constantly in the media. For example, a topic model in November would not have been able to predict the importance of topics associated with Coronavirus. This is especially problematic for geopolitical modelling, as often it would be events appearing out of the blue which cause the most amount of disruption.  

A similar problem exists for K Means. The KMeans algorithm for words in this instance uses the TFIDF vectorization to build clusters. This relies upon all of the words existing in corpus in a manner where the most important and common words appear most frequently. This means inherently the weighting is designed for a fully complete corpus, and not one which is being used to analyse new information. One of the main differences between the K Means algorithm and the LDA algorithm is that for K means it is possible to get a quantitative number for a word being close to a particular cluster, as Tf-IDF weightings can be calculated by inserting an offset for words which aren't in the corpus. However, this means that the distances to the cluster centres for such words are much less useful, as there is no way for an existing cluster model to tell apart a new rising topic, for example for a pre-covid trained model, words related to coronavirus or covid, from information which is irrelevant or noise, as they would both have the same calculated TF-IDF value with regards to the dataset.

As such this was the main reason the topic model was not used for filtering for a secondary dataset and why this approach in this fashion cannot be used accurately. 

Looking at the model results, it is difficult to say with any certainty what the best model is. The models were only trained on 2 months worth of data, which isn't sufficient to conclusively identify which model is the `best`, or at least most accurate. However, there are certain features of the models which become apparent, generally weighted models deal well with the data, the manually lagged non weighted model was certainly the worst off of all of the models which were tried. This does back up the theory that the Average Tone and the Goldstein Score may influence stock prices several days down the line, with the most important day being the last day in the moving window.

One of the other issues with this prediction interval is that the data restricts what the model has learned on. There is no guarantee that this would be representative over the much longer term, which would be more useful for investors.

One of the pertinent issues with prediction such as this is the importance to remember that in the long term the stock market always grows, thus this strategy would have to prove better than just investing in the long term growth of the stock market. 





