\section{Discussion}
\label{discussion}
The broader purpose of topic modelling was to build a topic model which would be able to categorise new and upcoming geopolitical risks, and thus be able to filter through only the relevant information. In the context of using GDELT, it would mean filtering out the headlines to only use the tone and Goldstein scale values of events where the headlines were related to the topic doing the filtering. This would mean that the topic model would not only have to be able to curate existing risks adequately, but also be able to catch new emerging risks too.

After performing the cluster analysis, and topic modelling, there were several things which became apparent. Firstly, the topic models are good at finding the main topics. Both the clusters and the LDA topic models were able to find what would be considered the `main` words in the topic, in that if a human were to look for the most important words, this is what they would be. Furthermore, if the TF-IDF results were examined, the results were surprisingly insightful into the data. The results were mostly coherent and representative of what a human would achieve for the time period of the training data.

However, one of the main concerns for both the LDA model and the clustering algorithm is the lack of separability. In both of the topic modelling attempts it appeared that there was only ever one topic being detected, in that there was no separation between the words such as `china` and `coronavirus`. This could perhaps be a result of there not being at least two topics present in the data. However, comparing the single day data and the USA China data, neither set of word clouds strongly show a prevailing topic, though both represent the data reasonably. This in itself is extremely promising, as the corpus and documents themselves were composed of URLs which were brute force parsed, which in turn meant that there was a lot of `garbage` data in the corpus, the result of parsed URLs where the headline was not coherent or even not there.

The main concern with the topic models is the fact that existing topic models cannot be used effectively in terms of filtering words, phrases, or topics unknown to the historical topic development, and thus cannot detect new topics. One of the biggest issues with LDA models is that the documents themselves have been generated from a specific set of topics, which means that if the word being filtered did not exist in the corpus there is no probability/closeness metric that can generated for each topic. This means that one of the only ways a topic model could be used for new data is if a dictionary was used to generate the corpus along with the news media at hand. This in turn would create two other problems. Firstly, it would disrupt the meaning of the topics as there would be an excess of unrelated words to the documents, this however could theoretically be solved by using extremely large datasets, and significantly larger amounts of processing power. The second issue with this approach is more prohibitive. New words and topics appear constantly in the media. For example, a topic model in November would not have been able to predict the importance of topics associated with Coronavirus. This is especially problematic for geopolitical modelling, as often it would be events appearing out of the blue which cause the most amount of disruption. This problem is not fixable by any amount of data or processing power, as no dataset can ever be complete indefinitely, new words/topics will always appear in the data.  

A similar problem exists for K-Means. The K-Means algorithm for words in this instance uses the TF-IDF vectorization to build clusters. This relies upon all of the words existing in corpus in a manner where the most common important words appear most frequently. This means inherently the weighting is designed for a fully complete corpus, and not one which is being used to analyse new information. One of the main differences between the K-Means algorithm and the LDA algorithm is that for K-Means it is possible to get a quantitative distance measure for a word not originally in the corpus to a particular cluster centre, as TF-IDF weightings can be calculated by inserting an offset for words which are not in the corpus. However, this means that the distances to the cluster centres for words not in the corpus are much less useful. There is no way for an existing cluster model to tell apart a new rising topic from information which is irrelevant or noise as both would have the same calculated TF-IDF value with regards to the corpus the TF-IDF is calculated from. For example a pre-covid trained cluster model, words related to coronavirus or covid would be categorised as noise or irrelevant instead of an upcoming rising topic.

This is shown in Figures \ref{fig:wordsk2}, \ref{fig:wordsk3}, and \ref{fig:wordsk4}. The results were plotted on a log scale, as distances between points for words and clusters can be significantly larger than the distances within clusters. Firstly, it becomes apparent that it does not matter which cluster is chosen in terms of calculating Mahalanobis distance for a new word or phrase, because all of the clusters are so close together, and the vectorised words are so far away. This means that when the log of the distance is taken, plotting the results mean that for each of the clusters the distance is exactly the same. 

Furthermore, some phrases do appear closer to any and all of the clusters than other phrases. This seems promising, as specifically the `coronavirus hits remote utah` is a relevant headline to the topic being detected, and thus it being closer suggests it might be useful. However, the difference on the log scale between that phrase and a totally irrelevant phrase (aboriginal peoples australia complain) is barely noticeable which furthers the idea that the TF-IDF is transforming relevant and `garbage` information in the same way. The single word `covid` is considerably closer, but a headline would rarely consist of only a single word, and more pertinently, in terms of relative distance for points within the cluster it is still orders of magnitude away. These plots also show that the low explained variance in the PCA decomposed dimensions isn't just a problem with the dimension picked, but rather is indicative of the clusters not being able to cluster the data effectively. 

As such this was the main reason the topic model was not used for filtering for a secondary dataset and why this approach in this fashion appears to not be useful in fashion for the task being attempted. Moreover, one of the biggest challenges with this work is that the topic models only work on a single word basis. All of the clusters and LDA models are based on single words, and not phrases, or content. This means there would still have to be a large amount of manual work required when using this approach, to ensure that the algorithms do not end up working with incoherence. The principle of garbage in garbage out also applies here, there is a substantial amount of preprocessing required to ensure that you're feeding the algorithm useful information, and even after that it is not guaranteed if the content is capable of making clusters or not.

Looking at the results from the stock prediction models, it is difficult to say with any certainty what the best model is. The models were only trained on 2 months worth of data, which is not sufficient to conclusively identify which model is the `best`, or at least most accurate. However, there are certain features of the models which become apparent, generally weighted models deal well with the data, the manually lagged non-weighted model was certainly the worst off of all of the models which were tried. This does back up the theory that the Average Tone and the Goldstein Score may influence stock prices several days down the line, with the most important day being the last day in the moving window. 

Looking at the model structures, the Random Forests classifiers did well with whatever style of smoothing performed on the data, suggesting that a Tree based approach may provide better results for this sort of problem. Examining the results of the `best` random forests model, it appears the model finds success in predicting upwards, and predicting small changes in the market. However, the model struggles with predicting shock falls in the stock market, which most likely would not be captured in any information available to the model.  

This shock also explains the difference in portfolios between the best Random Forests model and the reference model. The best Random Forests model around the middle of June completely missed the crash in the market, and held onto the stock which meant that in all of the strategies tried, the model wouldn't after that be able to catch up to the `reference` model. Interestingly, the reference model \textit{was} able to capture the fall in the middle of June, as in all three strategies, the model sold the stock, and thus didn't suffer a fall in total asset value. This reference model was only based on the previous day's prices, whereas the `best` model used the Average Tone and Goldstein Scale values as well. However, the previous day's feature importance was much less for the best model, shown in Figure \ref{fig:featimport}, so any internal pattern that the previous day was able to show to predict the fall, if present, may have been ignored as that feature was deemed less useful by the `best` model. 

One of the other issues with this prediction interval is that the data restricts what the model has learned on. The stock market went through remarkable changes over this period, and for large periods of time, there is no guarantee that this would be representative of regular behaviour on the stock market. Longer term more stable predictions would be more useful for investors looking to invest over the long term, as this strategy will over a longer period of time generally offer better results than short term investing.

It should also be noted that any analysis of potential monetary gain which might arise from using a model would have to be compared against stock market long term growth, as for the model to be fully useful, it would mean that it would be able to make gains \textit{beyond} the normal growth of the market. Interestingly, all of the portfolio strategies were able to make money in the time period which extrapolated could be comparable to the 9.5\% annual growth, and in one of the portfolio strategies, namely the stock maximising strategy, the total asset value at the end of the prediction period was in fact greater than the 9.5\% annual growth in just a two month period. This suggests that these models are in fact useful in terms of predicting the market, and could provide genuine monetary gain. 





