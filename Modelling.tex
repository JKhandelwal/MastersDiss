\section{Experiments}

The modelling and experiments of this dissertation were split into two parts. The first part explored different topic modelling approaches, in an aim to build and use a curated topic model, which could theoretically be used to filter out the relevant news from the GDELT data. The second part was to manually filter out a dataset of relevant data, and model stock shift against the Goldstein Scale and Average Tone values. These two approaches were run concurrently, with the eventual aim being that they would be merged if the initial experiments were successful, and then tested on another dataset, to get a measure of how accurate and useful this method might be. 

\subsection{Data Acquisition}
There were 2 main sources of data acquisition, firstly data was taken from the GDELT Events table. This data was for the use of building the topic model, and getting the average tone and Goldstein scale values for the events on specific days. The second dataset to be used was data for the Dow Jones Industrial average which measures the stock performance of 30 large companies on stock exchanges across the United States, namely the NASDAQ and the New York Stock Exchange.

\subsubsection{GDELT}
The GDELT data was acquired using two different procedures, firstly a python package was used to retrieve single data information. Then using the Google Cloud platform, and Google Big Query, the larger USA China Dataset was taken. There were two main reasons for selecting this subset of data. Firstly, there was significant news being generated between the USA and China during this period, and the stock market remained volatile, thus it seemed most promising to find a relationship, and build a curated topic model. Secondly, the events table data had to be restricted, as there would be too much data to process on the machine the models were run on, thus it had to be restricted. It should be noted that GDELT is a database which is updated on a daily basis with more and more backdated information. Thus the March and April data used in this dissertation was collected on the 11th of June 2020, and the May June data used for the predictive interval was collated on the 24th of July 2020. 

After the data was collected, There was a significant chunk of processing required for the GDELT data. Firstly the URLs had to be parsed to get the headlines (where present, not all of the URLs had headlines which were usable). Then the words were stripped and made into a corpus of documents, to be used in the topic modelling. 

The Goldstein scale and average tone values were aggregated across dates, and the results shown in Figures \ref{fig:avg_tone_diff} and \ref{fig:gs_diff}. There appears to be a lot of noise in individual dates, thus is a rolling average of 3 days is plotted, which appears to show more trends in the data. 

 
\subsubsection{Stock Market Data}
Stock Market data was acquired from the yahoo! finance United Kingdom website. For the stock market data, predictions were made on a daily basis. Since there were potentially lots of events made on each day, the average of the Goldstein scale and the Average Tone for all events on a specific day. There was no weighting of any events relative to each other, they were weighted exactly the same. The stock market daily difference is plotted in Figure \ref{fig:dow_diff}. The stock prices appeared to take a dive throughout March, which can be explained by the USA China trade flareups, and more pertinently, the impact of Covid-19. Interestingly, the overall trend of the stock data does mirror the Average tone considerably, and also slightly the Goldstein score. 

\subsection{Topic Modelling}

The initial topic modelling experiment was performed using several different sources. Firstly, a LDA model was run on single day GDELT event data, where there was no filtering performed. This was done as a simple initial experiment, to examine the results, and the topic separation. 

After this was completed, the next phase was to manually select data, and attempt to build a curated topic model. For this, GDELT data was selected using Google Big Query with the filters of being between the 1st of March and the 30th of April, and crucially for curation of the topic model, was restricted to only be events news about either the United States or China. The initial topic model used was the LDA model. The next stage was to use K Means clustering on  the USA China database. This meant that TFIDF was run on the USA/China GDELT data, and then K means was run on the data.

\subsubsection{Preprocessing}
%The first topic model was fitted on one day data, using the gensim package. 
All of topic models worked on the basis of using a corpus for the data, This meant that the URLs were parsed, and then headlines taken from them. Each headline was used as a document, and split into the requisite words to be used. In this process the text was cleaned to remove punctuation and special characters, and formatted to build a corpus full of documents, with each document being a list of words from one URL. 
\subsubsection{TFIDF}
The TFIDF vectoriser present in the sklearn package was used to calculate the vector encodings, and was used as the basis of the cluster analysis for k means clustering. It was run across the USA/China filtered data to get an initial understanding of which terms would be most important across the dataset.

\subsubsection{LDA}
To run the LDA model, the gensim package in the python programming language was used. Several LDA models were fitted, each with a different number of topics, to examine which would be the best number of topics for the data. After the data was fitted, several phrases were inserted in the LDA model to see the probability of those words being in that topic or not, though the LDA models restricted data to words which were already in the corpus, which meant they were not suitable as a filtering mechanism, as the whole purpose of filtering would be to only take the most relevant headlines from new data.

\subsubsection{K Means}
K means clustering was also fitted on the data. The package used was the K Means clustering algorithm already implemented in the sklearn python package. Several cluster numbers were tried, from fitting with 2 centroids to fitting with 4 centroids. The Mahalanobis distance and the euclidean distance was calculated for each clusters' points. Furthermore, sample phrases were tested against k means models, to see if the cluster was meaningfully able to recognise the difference between words related to the clusters and noise in the algorithms. From an implementation perspective, a random seed was set for each of the clusters so the centres would start at the same point, and thus the results would be reproducible. 
\subsection{Stock Modelling}

The main aim for this type of modelling was to predict whether the stock shifted up or down. Each day's difference was calculated between the opening and closing prices, and either a 1 or a 0 was decided to represent the stock market going up and down. For the scope of the project, and similar to other modelling approaches, it was decided to only predict either the market going up or down.

After finding the best model accuracy, the model was used to predict the differences between the 1st of May and the 30th of June, on the same set of data, i.e. the USA China GDELT filtered data. It was tested against the model which was just using the previous day's information to predict, and the accuracy presented against the days of prediction. The final model was compared to one which was just using the previous day's jump to predict the models. 

\subsubsection{Preprocessing}
There was substantial preprocessing required for the data, first of all the stock market does not open on weekends or other holidays, however news and events do. Thus, the news over weekends was collated and averaged into the Friday figures. This meant that the prediction data had to the shifted, to ensure that information from the future was not being used to predict the data.

The next issue to consider whilst preprocessing the data was the issue of lag modelling. It is reasonable to expect that if there is an underlying relationship between the Goldstein Score/Average Tone and the stock price, a specific day's stock price changes would not restricted to just the previous day's news, but instead could be a few days worth of modelling. Thus the average scores and the Goldstein scales would have to be smoothed using several moving window calculations.

\subsection{Models}
All of the models tried were classification models, as by reducing the stock price changes into up or down, it became binary data. There were 3 main models which were tried, a Naive Bayes model, a Random Forests Model, and a simple Logistic Regression. Furthermore, a simple multi layer neural network was also tried on the data to see if there was any underlying representation that could be learned. All of the models were trained using the Goldstein Scores and the Average Tone of each day's events, along with the previous day's stock change. 

Initially simple models were fitted with no moving average calculations necessary or done. Then, there were 3 different moving window types tried, firstly a simple average was taken where each day was weighted evenly. The second method of moving window was an exponential decay window, and similarly a half Gaussian moving window. Manual lags were also tried with no averaging, where the previous N days worth of data was used as part of the prediction. 