\section{Experiments}
\label{experiments}
The modelling and experiments of this dissertation were split into two parts. The first part explored different topic modelling approaches, in an aim to build and use a curated topic model. This topic model was trained on event data filtered in GDELT where the events were only those concerning the USA or China. This topic model could then be used on more data from GDELT, to filter out new events where the headlines from the URLs which match the topics used in the topic model. The same GDELT filtered USA/China dataset was used to model the binary stock shift with the Goldstein Scale and Average Tone values. 

These two approaches were run concurrently, with the eventual aim being that they would be merged if the initial experiments were successful, and then tested on another dataset, to get a measure of how accurate and useful this method might be. 

\subsection{Data Acquisition}
There were 2 main sources of data acquisition, firstly data was taken from the GDELT Events table. This data was for the use of building the topic model, and getting the average tone and Goldstein scale values for the events on specific days. The second dataset to be used was data for the Dow Jones Industrial average which measures the stock performance of 30 large companies on stock exchanges across the United States, namely the NASDAQ and the New York Stock Exchange.

\subsubsection{GDELT}
The GDELT data was acquired using two different procedures, firstly a python package was used to retrieve single data information. Then using the Google Cloud platform, and Google Big Query, the larger USA China Dataset was taken. There were two main reasons for selecting this subset of data. Firstly, there was significant news being generated between the USA and China during this period, and the stock market remained volatile, thus it seemed most promising to find a relationship, and build a curated topic model. Secondly, the events table data had to be restricted, as there would be too much data to process on the machine the models were run on. It should be noted that GDELT is a database which is updated on a daily basis with any missing backdated information being added. Thus the March and April data used in this dissertation was collected on the 11th of June 2020, and the May June data used for the predictive interval was collated on the 24th of July 2020. 

After the data was collected, There was a significant chunk of processing required for the GDELT data. Firstly the URLs had to be parsed to get the headlines (where present, not all of the URLs had headlines which were usable). Then the words were stripped and made into a corpus of documents, to be used in the topic modelling. 

The Goldstein scale and average tone values were aggregated across dates, and the results shown in Figures \ref{fig:avg_tone_diff} and \ref{fig:gs_diff}. There appears to be a lot of noise in individual dates, thus is a rolling average of 3 days is plotted, which appears to show more trends in the data. 

 
\subsubsection{Stock Market Data}
Stock Market data was acquired from the yahoo! finance United Kingdom website. For the stock market data, the model would predict whether the stock would rise or fall over an entire day. Since there were potentially lots of events occurring on each day, the average was taken of the Goldstein scale and the Average Tone for all events on a specific day. The stock market daily difference is plotted in Figure \ref{fig:dow_diff}. 

\subsection{Topic Modelling}

The initial topic modelling experiment was performed using several different sources. Firstly, a LDA model was run on single day GDELT event data, where there was no filtering performed. This was done as a simple initial experiment, to examine the results, and the topic separation. 

After this was completed, the next phase was to manually select data, and attempt to build a curated topic model. For this, GDELT data was selected using Google Big Query with the filters of being between the 1st of March and the 30th of April, and crucially for curation of the topic model, was restricted to only be events news about either the United States or China. The initial topic model used was the LDA model. The next stage was to use K Means clustering on  the USA China database. This meant that TFIDF was run on the USA/China GDELT data, and then K means was run on the data.

\subsubsection{Preprocessing}
%The first topic model was fitted on one day data, using the gensim package. 
All of topic models worked on the basis of using a corpus for the data, This meant that the URLs were parsed, and then headlines taken from them. Each headline was used as a document, and split into the requisite words to be used. In this process the text was cleaned to remove punctuation and special characters, and formatted to build a corpus full of documents, with each document being a list of words from one URL. 
\subsubsection{TFIDF}
The TFIDF vectoriser present in the sklearn package was used to calculate the vector encodings, and was used as the basis of the cluster analysis for k means clustering. It was run across the USA/China filtered data to get an initial understanding of which terms would be most important across the dataset. This was then used to numerically transform the headlines for the K Means algorithm. 

\subsubsection{LDA}
To run the LDA model, the gensim package in the python programming language was used. Several LDA models were fitted, each with a different number of topics, to examine which would be the best number of topics for the data. After the data was fitted, several phrases were inserted in the LDA model to see the probability of those words being in that topic or not. 

It was found that LDA models were only able to return a probability value for words which were already in the corpus. This meant that if there was a news headline related to the topics in the topic model but it contained a word not present in the corpus, it would mean that the topic model would not be able to say whether that news headline was part of that topic or not. This meant that this type of model could not be used to separate and filter irrelevant headlines from those headlines which were focused on the headlines in the model. 

\subsubsection{K Means}
K means clustering was also fitted on the data. The package used was the K Means clustering algorithm already implemented in the sklearn python package. Several cluster numbers were tried, from fitting with 2 centroids to fitting with 4 centroids. The Mahalanobis distance and the euclidean distance was calculated for each clusters' points. Furthermore, sample phrases were tested against k means models, to see if the cluster was meaningfully able to recognise the difference between words related to the clusters and noise in the algorithms. From an implementation perspective, a random seed was set for each of the clusters so the centres would start at the same point, and thus the results would be reproducible. 


\subsection{Stock Modelling}
The main aim for this type of modelling was to predict whether the stock shifted up or down over the course of a day. Each day's difference was calculated between the opening and closing prices, and either a 1 or a 0 was used to represent the stock market going up and down. For the scope of the project, and similar to other modelling approaches, it was decided to only predict either the market going up or down. There were several models which were tried, detailed in \ref{models}. In all cases the response variable being predicted is the change in the stock price over a day. This takes a value of 1/0 for either a fall or rise. This is denoted Y at time t (Yt), where t is a day, and ordered 1 to T, for the time interval used for training the model (April 1st to May 31st 2020). The predictors used were the Average Tone and Goldstein scale values for each day, along with differing amounts of predictors for previous day's data.

All of these models were tested and ranked on their accuracy. This was a percentage of the number of days that the model predicted whether the stock went up or down correctly. The `best` model was selected to be the one which had the highest accuracy values. The best model was then used to predict the stock shifts for the days between the 1st of May and the 30th of June, on the same set of data, i.e. the USA China GDELT filtered data. Another measure of the validity/accuracy of the best model was comparing it to a reference model. This reference model would use the same model type as the final model, but only use the previous day's stock shift, and \textit{not} the Average Tone or the Goldstein Scale values for predictions. 

\subsubsection{Preprocessing}
There was substantial preprocessing required for the data, first of all the stock market does not open on weekends or other holidays, however news and events do. Thus, the news over weekends was collated and averaged into the Friday figures. This meant that the prediction data had to the shifted, to ensure that information from the future was not being used to predict the data.

The next issue to consider whilst preprocessing the data was the issue of lag modelling. It is reasonable to expect that if there is an underlying relationship between the Goldstein Score/Average Tone and the stock price, a specific day's stock price changes would not restricted to just the previous day's news, but instead could be impacted by news over the previous several days. Thus the average scores and the Goldstein scales would have to be smoothed using several moving window calculations.

\subsection{Models}
\label{models}
All of the models tried were classification models, as by reducing the stock price changes into up or down, it became binary data. There were 4 main models which were tried, a Naive Bayes model, a Random Forests Model, a Support Vector Machine, and a simple Logistic Regression. Furthermore, a simple multi layer neural network was also tried on the data to see if there was any underlying representation that could be learned. All of the models were trained using 3 predictors, the Goldstein Score and the Average Tone of each day's events, and the previous day's stock change. 

Initially simple models were fitted with no moving average performed. However, it was assumed that an individual event could affect prices for several days, thus the Goldstein scale and Average tone predictors were smoothed using several different kinds of moving windows. There were 3 different moving window types tried, firstly a simple average was taken where each day was weighted evenly. The second method of moving window was an exponential decay window, and similarly a half Gaussian moving window. The moving windows were all spread over three days, as it was felt that this would provide the best balance at weighting past events evenly, without diluting the effect of the most recent news.

To test whether the previous N days' stock shift affected a specific day, three extra predictors were added, these predictors, along with the previous day's change, represented the stock shift for each of previous four days. These were used alongside the unsmoothed Average Tone and Goldstein Scale values to predict the stock prices. These models are henceforth referred to as the Manually Lagged Models.
