\section{Conclusions}
\label{conclusions}
There are many geopolitical events in the world, which affect markets. These are often represented and reported to people in various news sources. To be able to find and predict the impact such events would have on markets, these events and news sources would have to be treated mathematically. This would involve identifying the events determining the tone and impact of the news associated with these events. Then, to see how these events affect stock markets, build models on historical data linking markets to the event-tone and then automate the process such that it can be used as part of an advisory system for investors/shareholders in markets. 

This was distilled down to two features, firstly to be able to capture and cluster these events occurring in the news via topic models, and then use classical machine learning algorithms to use the quantitative values of the Tone and Goldstein Scale and predict shifts in the stock market. 

This dissertation aimed to use the GDELT database to build topic models from news headlines. If that were successful to aim was to use the topic model as a filter to collect relevant news which contained topics similar to the topic model. Then the eventual aim was to use the Conflict Cooperation Goldstein scale and the Average tone of the articles which covered these events as a geopolitical risk measure to predict changes in the Dow Jones index. 

There were several approaches to calculating geopolitical risk in the past which work from manually calculating occurrences of key words in news articles. This dissertation used a pre-existing Conflict Cooperation scale from events as a measure of geopolitical risk, with the main filtering occurring by choosing which events to factor in based on the headlines of those events.  The original attempt for topic modelling was to use a LDA model on restricted data to attempt to separate topics.    

It was found that this type of model was unable to provide a quantitative value for unseen information, as all information which could be classified would have to exist in the corpus when the topic model was created, which would mean that it cannot detect new topics occurring. This meant that this approach would not be usable.

Thus to try to gain a more quantitative approach, the K-means algorithm was used alongside TF-IDF vectorization, which would  firstly build clusters of words in an attempt to collate the individual topics present into those clusters. Then the objective was to use the Mahalanobis distance to filter out news and unseen headlines which are irrelevant to the topics in the cluster by calculating the distance to the cluster centres for the unseen headlines. The Mahalanobis distance was used as it provides a standardised measurement of the distance between, which isn't going to be affected by the shape of the cluster. 

However the main issue with this was that the distance metric for unseen headlines and news was not useful. The TF-IDF method meant that the algorithm placed new information the same as noise and irrelevant data. Thus this too would not be able to be used as it would not be able to distinguish new topics, nor was it able to provide distance calculations accurately for filtering purposes. 

Since neither of the topic modelling approaches appeared to work effectively, the modelling was done on the GDELT data filtered via Google Big Query in GDELT itself, as opposed to filtered through the topic models. The models used were classification models which aimed to predict stock shift up or down. These models appeared to be reasonably accurate, with the best of them being more accurate on a separate set of data as compared to a reference model which just used the previous day's data. The best of these models, when tested in a real world portfolio example, was even able to increase the portfolio value over the predictive period. Only a few model types and strategies were explored in this dissertation, there is definitely the potential in terms of further work to explore other different types of models.

Over the course of this dissertation, several topic modelling algorithms were built, but they ran into the problem of the topics and clusters being too close together, and thus neither the topics nor the clusters were distinct. More pertinently the issue of cluster and topic models being designed to work on a complete dataset, and not providing accurate results for unseen data was the main reason why this approach was thought to not work in this fashion. These algorithms might work if the data was more carefully selected with regards to TF-IDF values, but it does not appear to be useful in the current form. 

With regards to the stock predictive models, there always remains more scope for trying different types of models which may present more accurate results, but the results of the experiments appear to suggest that the Goldstein score and the Average tone might both be useful in terms of predicting on the stock market. Across the initial simple portfolio, the investors would be making money on the stock from the predictive models created, which suggests that this approach is feasible and viable for use in the real world.

\subsection{Further Work}
This dissertation was an attempt into seeing if the Goldstein scale alongside the Average Tone from the GDELT database can be used as a measure of geopolitical risk and calculated against the curated parts of the Stock Market. There is a large amount of further work which can be explored from this, in all aspects, from the topic modelling, to the modelling of Average Tone against the Goldstein Scale. 

One of the most apparent extensions of this work would be to try these methods on more data. GDELT is a large resource, and a vast amount of data is available to be explored and tried, from data from multiple other countries, to using more historical long term data. 

Furthermore, from these experiments alone it appears unlikely that topic modelling can be used in the way that it was initially thought in this dissertation. Even if the distance metrics are relatively accurate for new words and phrases, the topic models by design will inherently not be able to detect the presence of new topics, which would be a key aspect of this.

Another extension which could be performed is to use a method which works on more than just one word at a time. Both the TF-IDF K-Means and LDA model approaches work on a single word basis, which may not be enough to sufficiently separate the topics, thus an approach which takes into consideration phrases or bigrams may allow for more context to be explored which in turn may lead to better separability within the topics. 

One of the other main features which could be explored from a more modelling perspective could be an attempt to predict the stock shift in a more fine grained manner, this work only aimed to predict any changes in stock, and as a result this does not take magnitude of change into consideration, a large stock fall is the same as a small stock fall, but investor reactions may be different if it is only a minor fall in the market, thus being able to predict small, medium, or large shifts may become useful in the long term. Another factor which was not modelled was building autoregressive style lagging for the stock prices themselves, along with the lagged models for the Average Tone and the Goldstein Scale. 


\subsection{Final Remarks}

I feel this dissertation was able to explore several topic modelling avenues with respect to geopolitical news, and was able to show why this approach would not work with respect to news or topic filtering. However this dissertation was able to show that stock predicting is feasible from the geopolitical and tone information provided by GDELT. Even over a simple portfolio, this approach and this style of modelling appears to be profitable, and thus useful to investors in the real world, and this should be explored further. 

